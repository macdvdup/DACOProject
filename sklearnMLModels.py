# -*- coding: utf-8 -*-
"""Copy of CÃ³pia de Daco_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPwiJi5KEvfWkgooLqRDTpG1KwLfOODt

# Manuel Fortunato - DACO PROJECT - CNN vs ML algorthims
"""
import numpy as np
import torch

from BaseDataset import BaseDataset
import os

import matplotlib.pyplot as plt
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay



'''
FOR GOOGLE COLLAB
from google.colab import drive 
drive.mount('/gdrive')'''

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

path = os.getcwd() 
path = os.path.join(path,"datasets")

trainData= BaseDataset("new_TrainIn3.csv","new_TrainOut3.csv",path)
X_train = trainData.infoInput.to_numpy()
y_train_5d = trainData.infoOutput.to_numpy()  # shape (400,5)
y_train_1d = np.argmax(trainData.infoOutput.to_numpy(),1) # shape (400, )
print(y_train_1d.shape)

valData = BaseDataset("EvalIn3.csv","EvalOut3.csv",path)
X_val = valData.infoInput.to_numpy()
y_val_onehot = valData.infoOutput.to_numpy()
y_val = np.argmax(y_val_onehot,1)
print(y_val.shape)

testData= BaseDataset('TestIn3.csv','TestOut3.csv',path)
X_test = testData.infoInput.to_numpy()
y_test_5d = testData.infoOutput.to_numpy()
y_test_1d = np.argmax(testData.infoOutput.to_numpy(),1)
print(y_test_1d.shape)

output_classes = ('L. Foot', 'L. Hand','R. Foot','R. Hand','Tongue')


"""**(POPULAR) MACHINE LEARNING ALGORITHMS** -  using sklearn library

k-NEIGHBORS
"""

# define model
modelKNeigh = KNeighborsClassifier(n_neighbors=20)

# fit model
modelKNeigh.fit(X_train, y_train_1d)

# Test it
yhatmodelKNeigh = modelKNeigh.predict(X_test)

accmodelKNeigh = np.mean(yhatmodelKNeigh == y_test_1d)

print(accmodelKNeigh)

# determine the confusion matrix 
confMatrix = confusion_matrix(y_test_1d, yhatmodelKNeigh, normalize = None)
display = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = output_classes)
display= display.plot(cmap=plt.cm.Blues, xticks_rotation=0)
plt.title('Confusion Matrix - KNeighborsClassifier')



"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# define model
modelLogReg= LogisticRegression()

# fit model
modelLogReg.fit(X_train, y_train_1d)

# Test it
yhatLogReg = modelLogReg.predict(X_test)

accLogReg = np.mean(yhatLogReg == y_test_1d)

print(accLogReg)

# determine the confusion matrix
confMatrix = confusion_matrix(y_test_1d, yhatLogReg, normalize = None)
display = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = output_classes)
display= display.plot(cmap=plt.cm.Blues, xticks_rotation=0)
plt.title('Confusion Matrix - LogisticRegression')

"""Naive Bayes (Multinomial)

**Why can't negative data be used here?**

MultinomialNB assumes that features have multinomial distribution which is a generalization of the binomial distribution. Neither binomial nor multinomial distributions can contain negative values.
"""

def normalize(data):
    data_min = np.min(data)
    data_max = np.max(data)
    return (data - data_min) / (data_max - data_min)

modelMultNB = MultinomialNB()

# normalize array to values between [0 1]

normalized_X_train = normalize(X_train)
normalized_X_test = normalize(X_test)

# fit model
#vect = CountVectorizer()
#vect.fit(trainData.infoInput)
#input = vect.transform(trainData.infoInput)

modelMultNB.fit(normalized_X_train, y_train_1d)

# Test it
yhatMultNB = modelMultNB.predict(normalized_X_test)


accMultNB = np.mean(yhatMultNB == y_test_1d)

print(accMultNB)

# determine the confusion matrix
confMatrix = confusion_matrix(y_test_1d, yhatMultNB, normalize = None)
display = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = output_classes)
display= display.plot(cmap=plt.cm.Blues, xticks_rotation=0)
plt.title('Confusion Matrix - MultinomialNB')

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

# define model
modelDecTree = DecisionTreeRegressor()

# fit model
modelDecTree.fit(X_train, y_train_1d)

# Test it
yhatDecTree = modelDecTree.predict(X_test)

accDecTree = np.mean(yhatDecTree == y_test_1d)

print(accDecTree)

# determine the confusion matrix
confMatrix = confusion_matrix(y_test_1d, yhatDecTree, normalize = None)
display = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = output_classes)
display= display.plot(cmap=plt.cm.Blues, xticks_rotation=0)
plt.title('Confusion Matrix - DecisionTreeRegressor')

from sklearn import svm

model = svm.SVC(kernel='linear', C=1)

# fit model
model.fit(X_train, y_train_1d)

# Test it
yhat = model.predict(X_test)

acc = np.mean(yhat == y_test_1d)
print(yhat)

print(acc)

# determine the confusion matrix
confMatrix = confusion_matrix(y_test_1d, yhat, normalize = None)
display = ConfusionMatrixDisplay(confusion_matrix = confMatrix, display_labels = output_classes)
display= display.plot(cmap=plt.cm.Blues, xticks_rotation=0)
plt.title('Confusion Matrix - svm')
